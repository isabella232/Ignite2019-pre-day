{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices Prediction\n",
    "### Remote model training using Azure ML compute and Hyperdrive\n",
    "\n",
    "In this tutorial we will perform the same basic data preparation and model training steps as we did before, but know with the help of Azure ML service for remote model training and model hyperparameter search on Azure ML compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the necessary packages and setting some notebook options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os, time\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "import json\n",
    "config_file = open('config/ws_config.json')\n",
    "cred_dict = json.load(config_file)\n",
    "\n",
    "auth = ServicePrincipalAuthentication(tenant_id = cred_dict['tentant_id'], \n",
    "                                      service_principal_id = cred_dict['service_principal_id'], \n",
    "                                      service_principal_password = cred_dict['service_principal_password'])\n",
    "\n",
    "ws = Workspace.from_config(path=\"./config/ws_config.json\", auth = auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate an [Experiment](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiments) object, which will later be used to submit our model training execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name=\"house_prices_hyperdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to run our model training in a remote environment, we need to make the training data available to an external storage accessible to that environment . Here we are going to upload the training data to the [Default Datastore](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#datasets-and-datastores), which is available from our Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.upload(src_dir=\"./data\", target_path=\"data\", overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create our remote [Compute Target](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-targets).\n",
    "\n",
    "Here we create one of the type [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute). Once created, this resource is persisted and accessible by its name in subsequent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"compute01\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found existing compute target\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_DS2_V2\", \n",
    "                                                           max_nodes=12)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an [Estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#estimators) object, which facilitate the creation of run configurations, by defining [run scripts](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#training-scripts), its parameters and the target run environment.\n",
    "\n",
    "AML service provides a generic Estimator, as well as specialized ones that facilitate the usage of several popular python ML packages. Here we use the [SKLearn](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator?view=azure-ml-py) Estimator, as we are going to train Scikit-Learn based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = \"./scripts\"\n",
    "\n",
    "script_params = {\n",
    "    \"--data-folder\": ds.as_mount()\n",
    "}\n",
    "\n",
    "estimator = SKLearn(source_directory=script_folder, \n",
    "                    compute_target=compute_target,\n",
    "                    entry_script=\"train_model.py\",\n",
    "                    script_params=script_params\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to submit our Estimator for remote run on AML Compute. Instead of doing that directly, we will wrap it using the [HyperDrive](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) functionality for automated model hyperparameter search.\n",
    "\n",
    "The first step is to define how to sample the hyperparameter space. AML service provides several strategies already built in. Here we will use standard [Grid Sampling](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#sampling-the-hyperparameter-space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = GridParameterSampling({\n",
    "    \"n-estimators\": choice(500, 750, 1000),\n",
    "    \"max-depth\": choice(4, 6),\n",
    "    \"min-samples-split\": choice(2, 4),\n",
    "    \"learning-rate\": choice(0.01, 0.001)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the **Estimator** and grid sampling strategy, we can pass them to the [Hyper Drive configuration](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py) object. There are several options to configure here, such as the [termination policy](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-early-termination-policy), [resources](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#allocate-resources) to allocate the job on, and the [primary metric](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-primary-metric) to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n",
    "                                         hyperparameter_sampling=param_sampling,\n",
    "                                         policy=None,\n",
    "                                         primary_metric_name=\"test_MAE\",\n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MINIMIZE,\n",
    "                                         max_total_runs=100,\n",
    "                                         max_concurrent_runs=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining step is to submit the Experiment defined before, passing the configuration fot the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the execution through a Jupyter [graphical widget](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#visualize-experiment), available through the **RunDetails** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all executions generated by the hyperparameter search finish, we can inspect them and print the hyperparameters and correspondig model performance metrics in a table.\n",
    "\n",
    "Here this table is ordered by the best model according to the Mean Absolute Error computed for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "children = list(hyperdrive_run.get_children())\n",
    "metricslist = {}\n",
    "i = 0\n",
    "    \n",
    "for single_run in children:\n",
    "    results = {k: v for k, v in single_run.get_metrics().items() if isinstance(v, float)}\n",
    "    parameters = single_run.get_details()[\"runDefinition\"][\"arguments\"]\n",
    "    try:\n",
    "        results[\"n-estimators\"] = parameters[3]\n",
    "        results[\"max-depth\"] = parameters[5]\n",
    "        results[\"min-samples-split\"] = parameters[7]\n",
    "        results[\"learning-rate\"] = parameters[9]\n",
    "        metricslist[i] = results\n",
    "        i += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1).T.sort_values(by=[\"test_MAE\"], ascending=True)\n",
    "display(rundata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access directly the best run from our hyperdrive execution and then have access to the generated log files and the outputs we create explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files that we write to the special \"outputs\" folder are made available for each hyperdrive run. Here we list those generated by the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = best_run.get_file_names()\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then retrieve the model and corresponding train and test predictions that we explicitly saved in the run script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"./model\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "for f in file_names[-5:]:\n",
    "    best_run.download_file(f, model_folder)\n",
    "    \n",
    "model_name = file_names[-5].split(\"/\")[1]\n",
    "best_model = joblib.load(os.path.join(model_folder, model_name))\n",
    "\n",
    "y_pred_train = joblib.load(os.path.join(model_folder, file_names[-1].split(\"/\")[1]))\n",
    "y_true_train = joblib.load(os.path.join(model_folder, file_names[-2].split(\"/\")[1]))\n",
    "y_pred_test = joblib.load(os.path.join(model_folder, file_names[-3].split(\"/\")[1]))\n",
    "y_true_test = joblib.load(os.path.join(model_folder, file_names[-4].split(\"/\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute model performance metrics for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MAPE(y_actual, y_predict):\n",
    "    sum_actuals = sum_errors = 0\n",
    "    \n",
    "    for actual_val, predict_val in zip(y_actual, y_predict):\n",
    "        abs_error = actual_val - predict_val\n",
    "        if abs_error < 0:\n",
    "            abs_error = abs_error * -1\n",
    "\n",
    "        sum_errors = sum_errors + abs_error\n",
    "        sum_actuals = sum_actuals + actual_val\n",
    "\n",
    "    return sum_errors / sum_actuals\n",
    "\n",
    "print(\"MAPE (Train): %f\" % MAPE(y_true_train, y_pred_train))\n",
    "print(\"MAPE (Test): %f\" % MAPE(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"Acc (Train): %f\" % (1 - MAPE(y_true_train, y_pred_train)))\n",
    "print(\"Acc (Test): %f\" % (1 - MAPE(y_true_test, y_pred_test)))\n",
    "\n",
    "print(\"MAE (Train): %f\" % mean_absolute_error(y_true_train, y_pred_train))\n",
    "print(\"MAE (Test): %f\" % mean_absolute_error(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"R2 (Train): %f\" % r2_score(y_true_train, y_pred_train))\n",
    "print(\"R2 (Test): %f\" % r2_score(y_true_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the plots for predicted versus actual values for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y=y_pred_train, x=y_true_train)\n",
    "plt.plot(y_true_train, y_true_train, color=\"red\")\n",
    "plt.title(\"Predicted vs Actuals (Train)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(y=y_pred_test, x=y_true_test)\n",
    "plt.plot(y_true_train, y_true_train, color=\"red\")\n",
    "plt.title(\"Predicted vs Actuals (Test)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the error distributions for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_train - y_true_train)\n",
    "ax.set(title=\"Distribution of Errors (Train)\", xlabel=\"SalePrice\", ylabel=\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_test - y_true_test)\n",
    "ax.set(title=\"Distribution of Errors (Test)\", xlabel=\"SalePrice\", ylabel=\"frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ignite_ml_lab",
   "language": "python",
   "name": "ignite_ml_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
